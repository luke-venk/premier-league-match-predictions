{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904073b4-9494-4d06-b57b-ff8cc1857d9a",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "This notebook will be used in order to tune the hyperparameters for each of the models we are evaluating. After we determine the optimal set of hyperparameters for each model, we will update each model's source code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0465664e-0d2f-4582-8500-86eb0c42ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current file path and change the working directory to the project root\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.config import Models\n",
    "from src.util.DFImputer import DFImputer\n",
    "from src.util.notebook_utils import get_feature_matrix, weight_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66d6bb-0a1b-403c-8945-0b6b28c86212",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Load the dataset, with the configuration of N_MATCHES=5, with the training set including the 7 seasons from 2015/2016 to 2021/2022 and the holdout set including the 3 seasons from 2022/2023 to 2024/2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e184444-dfa5-492f-bb23-8dbf66782303",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_feature_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a130f0-8d23-45ae-a7bf-32076fb5a301",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parameter Grid to Search\n",
    "Here you should define the parameter grid for each of the models, so the program can search the best hyperparameters for each model. Use this program to determine the optimal hyperparameters to use for a given dataset, and update the src file accordingly.\n",
    "\n",
    "This section can be hidden by default since it's extremely long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e1ba9f-397b-4f7c-93fb-a09f5d328fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {    \n",
    "    Models.LOGISTIC_REGRESSION: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=5000\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"lbfgs\", \"saga\"]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.XGBOOST: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                objective=\"multi:softprob\",\n",
    "                num_class=3,\n",
    "                n_estimators=600,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                reg_lambda=1.0,\n",
    "                tree_method=\"hist\",\n",
    "                eval_metric=\"mlogloss\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [300, 600],\n",
    "            \"clf__max_depth\": [3, 4],\n",
    "            \"clf__learning_rate\": [0.05],\n",
    "            \"clf__subsample\": [0.9],\n",
    "            \"clf__colsample_bytree\": [0.8, 0.9],\n",
    "            \"clf__reg_lambda\": [0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.RANDOM_FOREST: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=600,\n",
    "                max_depth=None,\n",
    "                min_samples_leaf=2,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [300, 600, 900],\n",
    "            \"clf__max_depth\": [None, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.SVM: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", SVC(\n",
    "                kernel=\"rbf\",\n",
    "                C=2.0,\n",
    "                gamma=\"scale\",\n",
    "                probability=True,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.5, 1.0, 2.0, 4.0],\n",
    "            \"clf__kernel\": [\"rbf\", \"linear\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.MLPFFNN: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", MLPClassifier(\n",
    "                hidden_layer_sizes=(64, 32),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=1e-3,\n",
    "                learning_rate_init=1e-2,\n",
    "                max_iter=5000,\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__hidden_layer_sizes\": [(64,32), (128,64), (128,64,32)],\n",
    "            \"clf__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "            \"clf__learning_rate_init\": [1e-3, 1e-2],\n",
    "            \"clf__activation\": [\"relu\", \"tanh\"]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    Models.NAIVE_BAYES: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", GaussianNB(var_smoothing=1e-9))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0260a-c929-46e2-b933-928a1648c781",
   "metadata": {},
   "source": [
    "### Define the Model Tuning Function\n",
    "This function will automate the grid search to tune a hyperparameter for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34bd623-3215-47c4-b0cb-da32757df9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model_type: int, cv=5):\n",
    "    \"\"\"\n",
    "    Goes through the parameter grid configuration for a given model and determines\n",
    "    its best performing hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the model.\n",
    "        model: The model object corresponding to the model.\n",
    "        params: The hyperparameter configuration to search.\n",
    "        X_train: The features for the training data.\n",
    "        y_train: The labels for the training data.\n",
    "        cv: The number of cross-validation folds.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the model's best hyperparameters.\n",
    "    \"\"\"\n",
    "    print(f'Tuning {model_type}...')\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=param_grids[model_type]['model'],\n",
    "        param_grid=param_grids[model_type]['params'],\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=4,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f'Finished tuning {model_type} in {elapsed:.2f} seconds.')\n",
    "    print(f'Best score = {grid_search.best_score_:.5f}.')\n",
    "    print(f'Best params = {grid_search.best_params_}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c7ffe-c149-4473-98c7-04afeff814cc",
   "metadata": {},
   "source": [
    "### Running Grid Search Over Specific Models\n",
    "Each of the below cells will perform the grid search and print the best score and hyperparameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534027d9-2a73-474f-a6bd-59623dba0883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.LOGISTIC_REGRESSION...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Finished tuning Models.LOGISTIC_REGRESSION in 4.12 seconds.\n",
      "Best score = 0.58681.\n",
      "Best params = {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "tune_model(Models.LOGISTIC_REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca43ce9-6bdc-4e25-b02a-74baad6ffe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.XGBOOST...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Finished tuning Models.XGBOOST in 16.74 seconds.\n",
      "Best score = 0.57225.\n",
      "Best params = {'clf__colsample_bytree': 0.8, 'clf__learning_rate': 0.05, 'clf__max_depth': 3, 'clf__n_estimators': 300, 'clf__reg_lambda': 0.5, 'clf__subsample': 0.9}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "tune_model(Models.XGBOOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d07758-fcd7-4c08-b6c8-878caa177f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.RANDOM_FOREST...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Finished tuning Models.RANDOM_FOREST in 113.29 seconds.\n",
      "Best score = 0.57072.\n",
      "Best params = {'clf__max_depth': None, 'clf__min_samples_leaf': 1, 'clf__n_estimators': 900}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "tune_model(Models.RANDOM_FOREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167e9004-6cd1-4670-9f1e-328979bdf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.SVM...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Finished tuning Models.SVM in 26.40 seconds.\n",
      "Best score = 0.52587.\n",
      "Best params = {'clf__C': 0.5, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "tune_model(Models.SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeadf413-8101-4f51-a0cc-0ef1dc013218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.MLPFFNN...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Finished tuning Models.MLPFFNN in 228.93 seconds.\n",
      "Best score = 0.50939.\n",
      "Best params = {'clf__activation': 'tanh', 'clf__alpha': 0.01, 'clf__hidden_layer_sizes': (128, 64, 32), 'clf__learning_rate_init': 0.01}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MLP Feedforward Neural Network\n",
    "tune_model(Models.MLPFFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c29f223-006b-4850-805d-655a7364d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.NAIVE_BAYES...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Finished tuning Models.NAIVE_BAYES in 0.13 seconds.\n",
      "Best score = 0.47145.\n",
      "Best params = {'clf__var_smoothing': 1e-09}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "tune_model(Models.NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd457b5f-6409-41eb-a02a-d488c6582de7",
   "metadata": {},
   "source": [
    "### Tune the Voting Ensemble Weights\n",
    "After tuning all the individual models, update the models in the source code and in the Models.VOTING param_grids object, and tune the voting ensemble weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b654ad-384b-496d-931c-6c44db5376e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids[Models.VOTING] = {\n",
    "    \"model\": Pipeline([\n",
    "        (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "        (\"clf\", VotingClassifier(\n",
    "            estimators=[\n",
    "                (\"lr\", Pipeline([\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"clf\", LogisticRegression(\n",
    "                            solver='lbfgs',\n",
    "                            penalty='l2',\n",
    "                            C=0.1,\n",
    "                            max_iter=5000,\n",
    "                            random_state=0\n",
    "                        ))\n",
    "                    ])),\n",
    "                (\"xgb\", Pipeline([\n",
    "                        (\"clf\", XGBClassifier(\n",
    "                            objective=\"multi:softprob\",\n",
    "                            num_class=3,\n",
    "                            n_estimators=300,\n",
    "                            max_depth=3,\n",
    "                            learning_rate=0.05,\n",
    "                            subsample=0.9,\n",
    "                            colsample_bytree=0.8,\n",
    "                            reg_lambda=1.0,\n",
    "                            tree_method=\"hist\",\n",
    "                            eval_metric=\"mlogloss\",\n",
    "                            random_state=0,\n",
    "                            n_jobs=-1\n",
    "                        ))\n",
    "                    ])),\n",
    "                (\"rf\", Pipeline([\n",
    "                        (\"clf\", RandomForestClassifier(\n",
    "                            n_estimators=900,\n",
    "                            max_depth=None,\n",
    "                            min_samples_leaf=1,\n",
    "                            class_weight=\"balanced\",\n",
    "                            random_state=0\n",
    "                        ))\n",
    "                    ]))\n",
    "            ],\n",
    "            voting=\"soft\",\n",
    "            n_jobs=1\n",
    "        ))\n",
    "    ]),\n",
    "    \"params\": {\"clf__weights\": weight_vectors()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e327a6-8788-440b-8c1c-563e58af75a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Models.VOTING...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Finished tuning Models.VOTING in 202.57 seconds.\n",
      "Best score = 0.58605.\n",
      "Best params = {'clf__weights': [3, 1, 1]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tune_model(Models.VOTING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
