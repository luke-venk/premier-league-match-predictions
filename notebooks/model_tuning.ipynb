{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465664e-0d2f-4582-8500-86eb0c42ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/code/premier-league-match-predictions'\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from src.util.DFImputer import DFImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from src.config import Models\n",
    "from src.config import END_YEAR, NUM_SEASONS, SPORTSBOOK, N_MATCHES\n",
    "from src.data.load_data import load_all_seasons\n",
    "from src.data.build_features import build_rolling_features\n",
    "from src.data.split import chrono_split\n",
    "from src.models.hyperparameter_search import search_models, tune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e184444-dfa5-492f-bb23-8dbf66782303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = load_all_seasons(end_year=END_YEAR, num_seasons=NUM_SEASONS, sportsbook=SPORTSBOOK)\n",
    "df = build_rolling_features(df=df_raw, n_matches=N_MATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46c8c0-8eaf-4397-af1f-c46dcc03203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = chrono_split(df, train_ratio=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a130f0-8d23-45ae-a7bf-32076fb5a301",
   "metadata": {},
   "source": [
    "### Parameter Grid to Search\n",
    "Here you should define the parameter grid for each of the models, so the program can search the best hyperparameters for each model. Use this program to determine the optimal hyperparameters to use for a given dataset, and update the src/ directory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1ba9f-397b-4f7c-93fb-a09f5d328fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "\n",
    "    Models.NAIVE_BAYES: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", GaussianNB(var_smoothing=1e-9))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    Models.LOGISTIC_REGRESSION: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=5000\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"lbfgs\", \"saga\"]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.RANDOM_FOREST: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=600,\n",
    "                max_depth=None,\n",
    "                min_samples_leaf=2,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [300, 600, 900],\n",
    "            \"clf__max_depth\": [None, 10, 20],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.SVM: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", SVC(\n",
    "                kernel=\"rbf\",\n",
    "                C=2.0,\n",
    "                gamma=\"scale\",\n",
    "                probability=True,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.5, 1.0, 2.0, 4.0],\n",
    "            \"clf__kernel\": [\"rbf\", \"linear\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.MLPFFNN: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", MLPClassifier(\n",
    "                hidden_layer_sizes=(64, 32),\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=1e-3,\n",
    "                learning_rate_init=1e-2,\n",
    "                max_iter=5000,\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__hidden_layer_sizes\": [(64,32), (128,64), (128,64,32)],\n",
    "            \"clf__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "            \"clf__learning_rate_init\": [1e-3, 1e-2],\n",
    "            \"clf__activation\": [\"relu\", \"tanh\"]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    Models.XGBOOST: {\n",
    "        \"model\": Pipeline([\n",
    "            (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                objective=\"multi:softprob\",\n",
    "                num_class=3,\n",
    "                n_estimators=600,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                reg_lambda=1.0,\n",
    "                tree_method=\"hist\",\n",
    "                eval_metric=\"mlogloss\",\n",
    "                random_state=0\n",
    "            ))\n",
    "        ]),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [300, 600],\n",
    "            \"clf__max_depth\": [3, 4],\n",
    "            \"clf__learning_rate\": [0.05],\n",
    "            \"clf__subsample\": [0.9],\n",
    "            \"clf__colsample_bytree\": [0.8, 0.9],\n",
    "            \"clf__reg_lambda\": [0.5, 1.0]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c7ffe-c149-4473-98c7-04afeff814cc",
   "metadata": {},
   "source": [
    "# Running Grid Search Over Specific Models\n",
    "This section will be used if just want to explore the best parameters for one model type, since this is much quicker than comparing all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02b356-82f8-44c6-8893-0ac79fd55ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_grid_search(model_type):\n",
    "    tune_model(\n",
    "        name=model_type,\n",
    "        model=param_grids[model_type]['model'],\n",
    "        params=param_grids[model_type]['params'],\n",
    "        X_train=X_train,\n",
    "        y_train=y_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534027d9-2a73-474f-a6bd-59623dba0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "individual_grid_search(Models.LOGISTIC_REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca43ce9-6bdc-4e25-b02a-74baad6ffe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "individual_grid_search(Models.XGBOOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d07758-fcd7-4c08-b6c8-878caa177f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "individual_grid_search(Models.RANDOM_FOREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a432d-eb17-4d5e-a3a0-695500228f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting\n",
    "individual_grid_search(Models.VOTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583244f-3683-48af-8594-674eb7816b7a",
   "metadata": {},
   "source": [
    "# Running the Entire Grid Search Over All Models\n",
    "This automates running the grid search for each of the models and sorting by accuracy, but takes a lot longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f982ea-7912-425f-83ab-c9e08fc9fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searches = search_models(param_grids, X_train, y_train)\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': grid_search['name'],\n",
    "        'Best Accuracy': grid_search['best_score'],\n",
    "        'Best Params': grid_search['best_params']\n",
    "    }\n",
    "    for grid_search in grid_searches\n",
    "])\n",
    "\n",
    "summary_df.sort_values('Best Accuracy', ascending=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd457b5f-6409-41eb-a02a-d488c6582de7",
   "metadata": {},
   "source": [
    "# Voting\n",
    "After tuning all the individual models, update the models in the source code and in the following param_grids object, and tune the voting classification weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e327a6-8788-440b-8c1c-563e58af75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight_vectors(num=20, k=4):\n",
    "    \"\"\"\n",
    "    Generate <num> vectors of size <k> to tune weights in the\n",
    "    voting model.\n",
    "    \"\"\"\n",
    "    weights = np.random.randint(1, k + 1, size=(num, k))\n",
    "    return [list(weight) for weight in weights]\n",
    "\n",
    "\n",
    "# TODO: update with tuned hyperparameters\n",
    "voting_params = {\n",
    "    \"model\": Pipeline([\n",
    "        (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "        (\"clf\", VotingClassifier(\n",
    "            estimators=[\n",
    "                (\"lr\", Pipeline([\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"clf\", LogisticRegression(\n",
    "                            solver=\"lbfgs\",\n",
    "                            max_iter=5000,\n",
    "                            C=1.0,\n",
    "                            random_state=0\n",
    "                        ))\n",
    "                    ])),\n",
    "                (\"rf\", Pipeline([\n",
    "                        (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "                        (\"clf\", RandomForestClassifier(\n",
    "                            n_estimators=500,\n",
    "                            max_depth=None,\n",
    "                            max_features=\"sqrt\",\n",
    "                            n_jobs=-1,\n",
    "                            random_state=0\n",
    "                        ))\n",
    "                    ])),\n",
    "                (\"hgb\", Pipeline([\n",
    "                        (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "                        (\"clf\", HistGradientBoostingClassifier(\n",
    "                            loss=\"log_loss\",\n",
    "                            learning_rate=0.06,\n",
    "                            max_depth=6,\n",
    "                            max_iter=600,\n",
    "                            random_state=0\n",
    "                        ))\n",
    "                    ])),\n",
    "                (\"xgb\", Pipeline([\n",
    "                        (\"imputer\", DFImputer(strategy=\"median\")),\n",
    "                        (\"clf\", xgb.XGBClassifier(\n",
    "                            objective=\"multi:softprob\",\n",
    "                            num_class=3,\n",
    "                            n_estimators=500,\n",
    "                            learning_rate=0.06,\n",
    "                            max_depth=6,\n",
    "                            subsample=0.9,\n",
    "                            colsample_bytree=0.9,\n",
    "                            reg_lambda=1.0,\n",
    "                            tree_method=\"hist\",\n",
    "                            n_jobs=-1,\n",
    "                            random_state=0,\n",
    "                            eval_metric=\"mlogloss\",\n",
    "                            verbosity=0\n",
    "                        ))\n",
    "                    ])),\n",
    "            ],\n",
    "            voting=\"soft\",\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \"params\": {\n",
    "        \"clf__weights\": random_weight_vectors()\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
